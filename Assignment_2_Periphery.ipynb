{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RafaelNovais/Python/blob/master/Assignment_2_Periphery.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06NNXcAnTGfI"
      },
      "source": [
        "# CT5132 / CT5148 Assignment 2: Periphery\n",
        "\n",
        "**Summary**: In this assignment, you have to investigate, explain, and implement one or more methods of finding the **periphery** of a dataset. The periphery doesn't have a single definition, so you have some freedom to look at different definitions and approaches.\n",
        "\n",
        "You are allowed to use any sources and resources you like, including code and documents from the internet, existing libraries, and generative AI methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZDbpP0ETGfL"
      },
      "source": [
        "**Due date**: as announced on Canvas.\n",
        "\n",
        "**Weight**: this assignment is worth 20% of the module.\n",
        "\n",
        "**Groups**: you may work in a group of 1 (solo), 2, or 3, but you must not work with any student you previously worked with in any assignment in this module or any other module.\n",
        "\n",
        "If working in a group:\n",
        "* You must notify the lecturer by email 2 weeks before the due date, cc-ing all students in the group, and state that you have not worked together in any other assignment.\n",
        "* Students in a group must work together on all aspects -- you cannot split tasks up so that one student doesn't understand how another student's part of the project works.\n",
        "* All students in the group must submit and the submissions must be identical.\n",
        "* All communication should cc all group members.\n",
        "\n",
        "**Resources**:\n",
        "\n",
        "* You may use code you find on the internet, but you must clearly cite the URL and clearly mark which parts of your solution are from that URL.\n",
        "* You may use code completion and generative AI methods, such as GitHub Copilot and ChatGPT. When doing so you must clearly document your methodology when using it, eg the system(s) you use, the prompts, how you notice when the AI gets things wrong, how you improve it, etc. You must clearly document which parts of your submission come from generative AI.\n",
        "* You may discuss with other students/groups, but you may not show your work to them or view their work.\n",
        "\n",
        "**Interviews**: Post-submission vivas will be used in some cases to give students an opportunity to demonstrate their learning.\n",
        "\n",
        "**Grading**: A basic solution (one simple method, based on a single source, demonstrated on one dataset, with short explanation) will be enough for a pass grade. For a very high grade, you can add more value by, for example:\n",
        "\n",
        "* Investigating multiple methods,\n",
        "* describing your methods correctly,\n",
        "* implementing and comparing them (results, visualisations, pros and cons),\n",
        "* on multiple datasets with different properties;\n",
        "* providing a sophisticated account of the use of generative AI;\n",
        "* integrating information from multiple sources;\n",
        "* implementing your methods as Scikit-Learn estimators (NB: for some approaches to the problem there is no \"right answer\", so we may or may not need to implement `score`),\n",
        "* with clean code and appropriate documentation and comments;\n",
        "* with vectorisation where appropriate;\n",
        "* describing the run-time or computational complexity.\n",
        "\n",
        "These are examples of how you can add value, not an exhaustive list.\n",
        "\n",
        "**NB**: some approaches to the problem are based on machine learning, but others are not. This is not a machine learning module, so you are not obliged to choose any machine learning methods.\n",
        "\n",
        "There is no lower or upper bound on word-count or line-count. However, a high-quality submission with 3 methods and 3 datasets could be done in less than 100 lines and less than 1000 words.\n",
        "\n",
        "**Submission**: submit an `ipynb` file containing code, results, and explanations. If you need to submit any data files as well, you can submit a `zip` file. Include your name and ID."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rafael Novais de Melo -\n",
        "23113607**"
      ],
      "metadata": {
        "id": "XoDBgUxtdcpv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsK6oiuNTGfN"
      },
      "source": [
        "### Problem definition\n",
        "\n",
        "Given a dataset of points `X` of shape `(n_samples, n_features)`, the **periphery** is a subset of those points which are in some sense \"on the border\", or \"outside the main dataset\". The image below shows one possible conception of the periphery. I created it by hand for this dataset. Your job is to investigate methods of doing it automatically for any dataset. Some relevant ideas include the **convex hull**, **anomaly detection**, **outliers**, **bounding box**.\n",
        "\n",
        "(Why is the periphery interesting? One motivation is when investigating the performance of regression and classification models in **extrapolation**: if we train only on the core, how is performance on the unseen periphery? But this is just motivation: we are not required to investigate extrapolation for this assignment.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNv82v5PTGfN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.base import BaseEstimator\n",
        "# Create a random dataset\n",
        "np.random.seed(1)\n",
        "X = np.random.rand(30, 2)\n",
        "# Create labels: 0 => core, 1 => periphery\n",
        "labels = np.zeros(len(X))\n",
        "# by hand!\n",
        "labels[[1, 10, 12, 13, 14, 19, 20]] = 1\n",
        "core = X[labels == 0]\n",
        "periphery = X[labels == 1]\n",
        "plt.scatter(core[:, 0], core[:, 1], c='blue', label='Core')\n",
        "plt.scatter(periphery[:, 0], periphery[:, 1], c='red', label='Periphery')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flWkipQVTGfT"
      },
      "outputs": [],
      "source": [
        "#Read and import file\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial import ConvexHull\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import seaborn as sns\n",
        "from scipy.spatial import ConvexHull\n",
        "import requests\n",
        "from io import StringIO\n",
        "\n",
        "#df = pd.read_csv('/content/drive/MyDrive/Master/Courses/pandas-completo/Data Science/Beer.csv')\n",
        "#url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "\n",
        "#Validation Data Prepared\n",
        "df = upload_file()\n",
        "selected_column = select_column(df)\n",
        "\n",
        "\n",
        "#Function\n",
        "print('Convex Hull')\n",
        "convex_hull(selected_column)\n",
        "print('Isolation Forest')\n",
        "isolation_forest(selected_column)\n",
        "print('Bounding Box')\n",
        "bounding_box(selected_column)\n",
        "print('Outliers')\n",
        "outliers(selected_column)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Upload File from ChatGpt\n",
        "def upload_file():\n",
        "    file_path_or_url = input(\"Please enter the file path or URL to upload: \")\n",
        "\n",
        "    try:\n",
        "        if file_path_or_url.startswith(\"http\"):\n",
        "            # It's a URL, so fetch the data from the URL\n",
        "            response = requests.get(file_path_or_url)\n",
        "            response.raise_for_status()\n",
        "            content = response.text\n",
        "            df = pd.read_csv(StringIO(content))\n",
        "        else:\n",
        "            # It's a local file, so read it as a CSV\n",
        "            df = pd.read_csv(file_path_or_url)\n",
        "\n",
        "        return df\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(\"Failed to fetch data from the URL:\", str(e))\n",
        "    except FileNotFoundError:\n",
        "        print(\"File not found. Please enter a valid file path.\")\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(\"The file is empty or has no data.\")\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred:\", str(e))\n"
      ],
      "metadata": {
        "id": "QkMhm3lnUtj9"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This was my upload file, but didn't work with URL and Path so i asked to ChatGPT hHow can i Import a csv file from a path or from a URL in the same fuction, and he gave me this code.\n",
        "\n",
        "\n",
        "```\n",
        "def upload_file(): #Upload/Read csv file\n",
        "    file_path = input(\"Please enter the file path or URL to upload: \")\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df\n",
        "```\n"
      ],
      "metadata": {
        "id": "0bi9bY4opvhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_column(dataframe): # Function to select 2 columns in the data to identify the periphery\n",
        "\n",
        "        print(\"Columns in the DataFrame:\") #Show all columns names to select 2\n",
        "        for column in dataframe.columns:\n",
        "            print(column)\n",
        "\n",
        "\n",
        "        selected_columns = [] #Select 2 columns from the dataset and inclued in a new dataframe\n",
        "        while len(selected_columns) < 2:\n",
        "            column_name = input(\"Enter the name of a column: \")\n",
        "            if column_name in dataframe.columns:\n",
        "                selected_columns.append(column_name)\n",
        "            else:\n",
        "                print(\"Please enter a valid column name.\")\n",
        "\n",
        "\n",
        "        selected_df = dataframe[selected_columns]\n",
        "\n",
        "        return selected_df"
      ],
      "metadata": {
        "id": "urdAYrwcUyKg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ConvexHull\n",
        "def convex_hull(data):\n",
        "    xy = data.values\n",
        "    hull = ConvexHull(xy) # Convex Hull is a function where the next convex angle is found.\n",
        "    periphery_indices = hull.vertices #Set the index os all periphery in a variable\n",
        "    periphery_indices = periphery_indices.astype(int)\n",
        "    periphery_points = data.iloc[periphery_indices] #Set the all periphery points in the graphic in a variable\n",
        "    plt.scatter(data.iloc[:, 0], data.iloc[:, 1], s=50, c='b', label='Main') #Plot the all the points in the graphic\n",
        "    plt.scatter(periphery_points.iloc[:, 0], periphery_points.iloc[:, 1], s=50, c='r', label='Periphery') # Paint the periphery in red and save in graphic\n",
        "    plt.plot(data.iloc[periphery_indices, 0], data.iloc[periphery_indices, 1], 'k--', lw=2)#Conect all the periphery point with line and save in graphic\n",
        "    plt.legend()# Display the graphic\n",
        "    plt.title('Convex Hull')\n",
        "    plt.xlabel(data.columns[0])\n",
        "    plt.ylabel(data.columns[1])\n",
        "    plt.show()# Display the graphic\n",
        "\n"
      ],
      "metadata": {
        "id": "pfIOIBICbS5X"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#IsolationForest\n",
        "def isolation_forest(data):\n",
        "\n",
        "  model = IsolationForest(contamination=0.05)  # Adjust contamination based on your dataset\n",
        "  model.fit(data) # Fit the model to the data\n",
        "  outliers = model.predict(data) # Predict anomalies (outliers)\n",
        "  anomaly_points = data[outliers == -1] # Extract the anomaly points\n",
        "  plt.scatter(data.iloc[:, 0], data.iloc[:, 1], s=50, c='b', label='Normal Data')\n",
        "  plt.scatter(anomaly_points.iloc[:, 0], anomaly_points.iloc[:, 1], s=50, c='r', label='Anomaly Points')\n",
        "  plt.legend()\n",
        "  plt.xlabel(data.columns[0])\n",
        "  plt.ylabel(data.columns[1])\n",
        "  plt.title('Isolation Forest')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "7IZEtbusJB0T"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def outliers(data):\n",
        "  plt.figure(figsize=(20, 10))\n",
        "  sns.boxplot(x=data[data.columns[0]], y=data[data.columns[1]], color='lightblue', width=0.6)\n",
        "  sns.stripplot(x=data[data.columns[0]], y=data[data.columns[1]], color='red', size=4, jitter=0.2)\n",
        "  plt.xlabel(data.columns[0])\n",
        "  plt.ylabel(data.columns[1])\n",
        "  plt.title(f\"Outliers in {data.columns[0]} vs {data.columns[1]}\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "FT5iW2HqjZve"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bounding Box using ChatGPT\n",
        ">\n",
        "Rafael : can you do an exemple of bounding box with python.\n",
        ">\n",
        "ChatGPT: Generated\n",
        "```\n",
        "# Extract the feature columns\n",
        "# Calculate the minimum and maximum values for each feature\n",
        "# Define the bounding box\n",
        "```\n",
        "Rafael: how can i do a plot with the anwser in Bounding Box:\n",
        "```\n",
        "# Filter points within the bounding box\n",
        "# Create a scatter plot for all data points\n",
        "# Create a rectangle for the bounding box\n",
        "# Plot the\n",
        "# Highlight points within the bounding box\n",
        "```\n",
        "Rafael: ValueError: Can only compare identically-labeled Series objects\n",
        "---> 19     (X[\"temp_max\"] >= bounding_box[\"min_temp_max\n",
        ">\n",
        "ChatGPT: Fixed the Code, and return the atual code\n",
        "```\n",
        "This corrected code should allow you to create a plot with the bounding box and points within the box for your dataset. Make sure to replace \"dataset.csv\" with the path to your CSV file and adjust the column names as needed.\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LD8CE_FINK8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bounding Box from ChatGPT\n",
        "def bounding_box(data):\n",
        "    # Extract the feature columns\n",
        "    X = data\n",
        "\n",
        "    # Calculate the minimum and maximum values for each feature\n",
        "    min_vals = X.min()\n",
        "    max_vals = X.max()\n",
        "\n",
        "    # Define the bounding box using column names\n",
        "    bounding_box = {\n",
        "        f'min_{X.columns[0]}': min_vals[0],\n",
        "        f'max_{X.columns[0]}': max_vals[0],\n",
        "        f'min_{X.columns[1]}': min_vals[1],\n",
        "        f'max_{X.columns[1]}': max_vals[1]\n",
        "    }\n",
        "\n",
        "    # Filter points within the bounding box\n",
        "    points_within_box = X[\n",
        "        (X[X.columns[0]] >= bounding_box[f'min_{X.columns[0]}']) &\n",
        "        (X[X.columns[0]] <= bounding_box[f'max_{X.columns[0]}']) &\n",
        "        (X[X.columns[1]] >= bounding_box[f'min_{X.columns[1]}']) &\n",
        "        (X[X.columns[1]] <= bounding_box[f'max_{X.columns[1]}'])\n",
        "    ]\n",
        "\n",
        "    # Create a scatter plot for all data points\n",
        "    plt.scatter(X[X.columns[0]], X[X.columns[1]], c='b', label='Data Points')\n",
        "\n",
        "    # Create a rectangle for the bounding box\n",
        "    rectangle = plt.Rectangle(\n",
        "        (bounding_box[f'min_{X.columns[0]}'], bounding_box[f'min_{X.columns[1]}']),\n",
        "        bounding_box[f'max_{X.columns[0]}'] - bounding_box[f'min_{X.columns[0]}'],\n",
        "        bounding_box[f'max_{X.columns[1]}'] - bounding_box[f'min_{X.columns[1]}'],\n",
        "        color='r',\n",
        "        fill=False,\n",
        "        label='Bounding Box'\n",
        "    )\n",
        "\n",
        "    # Plot the rectangle\n",
        "    plt.gca().add_patch(rectangle)\n",
        "\n",
        "    # Highlight points within the bounding box\n",
        "    plt.scatter(points_within_box[X.columns[0]], points_within_box[X.columns[1]], c='g', label='Points in Bounding Box')\n",
        "\n",
        "    plt.xlabel(X.columns[0])\n",
        "    plt.ylabel(X.columns[1])\n",
        "    plt.legend()\n",
        "    plt.title(f\"Bounding Box and Data Points for {X.columns[0]} and {X.columns[1]}\")\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "Mzqfg0n67P8a"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTqQuKZQLCF0",
        "outputId": "fd8f0f0f-6e8c-4d56-ce5f-786089f9a78c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Periphery of a dataset is important for assessing a model's generalization, robustness, and its ability to handle unseen or extreme data points. It has practical applications in various domains, including predictive modeling, anomaly detection, and fairness in machine learning."
      ],
      "metadata": {
        "id": "nhvmp8Wnipvy"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py39",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}